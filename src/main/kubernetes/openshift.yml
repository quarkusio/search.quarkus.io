---
apiVersion: v1
kind: Service
metadata:
  name: search-backend
  labels:
    app: search-backend
    app.kubernetes.io/name: search-backend
    app.kubernetes.io/component: datastore
    app.kubernetes.io/part-of: '{{ .Values.app.name }}'
    app.kubernetes.io/version: '{{ .Values.app.version }}'
spec:
  ports:
    - name: http
      port: 9200
      protocol: TCP
    - name: inter-node
      protocol: TCP
      port: 9300
  selector:
    app.kubernetes.io/name: search-backend
  type: ClusterIP
  # Using a StatefulSet, each pod has its own immutable address,
  # so we don't need the service to have an IP.
  clusterIP: None
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: search-backend
  labels:
    app: search-backend
    app.kubernetes.io/name: search-backend
    app.kubernetes.io/component: datastore
    app.kubernetes.io/part-of: '{{ .Values.app.name }}'
    app.kubernetes.io/version: '{{ .Values.app.version }}'
# See https://www.hafifbilgiler.com/hafif-bilgiler/elasticsearch-installation-on-openshift/
spec:
  serviceName: search-backend
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: search-backend
  template:
    metadata:
      labels:
        app: search-backend
        app.kubernetes.io/name: search-backend
        app.kubernetes.io/component: datastore
        app.kubernetes.io/part-of: '{{ .Values.app.name }}'
        app.kubernetes.io/version: '{{ .Values.app.version }}'
      annotations:
        alpha.image.policy.openshift.io/resolve-names: '*'
    spec:
      containers:
        - name: elasticsearch
          # The image gets pushed manually as part of the "deploy" workflow.
          # This gets replaced with the correct image ref (exact tag).
          image: elasticsearch-custom:latest
          imagePullPolicy: Always
          resources:
            limits:
              cpu: '{{ .Values.elasticsearch.resources.limits.cpu }}'
              memory: '{{ .Values.elasticsearch.resources.limits.memory }}'
            requests:
              cpu: '{{ .Values.elasticsearch.resources.requests.cpu }}'
              memory: '{{ .Values.elasticsearch.resources.requests.memory }}'
          readinessProbe:
            httpGet:
              scheme: HTTP
              path: /_cluster/health?local=true
              port: 9200
            initialDelaySeconds: 5
          ports:
            - name: http
              containerPort: 9200
              protocol: TCP
            - name: inter-node
              containerPort: 9300
              protocol: TCP
          volumeMounts:
            - name: data
              mountPath: /usr/share/elasticsearch/data
          env:
            - name: cluster.name
              value: search-quarkus-io
            - name: node.name
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            # We don't have enough nodes/memory available in the cluster to allow for 3 decently-sized pods,
            # and 3 pods with low memory perform badly, so we'll have to make do with a single pod.
            - name: discovery.type
              value: "single-node"
            # Memory locking doesn't work on our OpenShift instance,
            # but this shouldn't be too bad as we don't expect swapping to be enabled.
            - name: bootstrap.memory_lock
              value: "false"
            # Set the -Xmx explictly and don't rely on the search backend to figure out memory limits on its own.
            - name: ES_JAVA_OPTS
              value: '{{ .Values.elasticsearch.envs.ES_JAVA_OPTS }}'
            # Not exposed to the internet, no sensitive data
            # => We don't bother with HTTPS and pesky self-signed certificates
            # Setting this env variable is better than setting plugins.security.disabled
            # because this skips installing the plugin altogether (see above)
            - name: xpack.security.enabled
              value: 'false'
            # Disable disk-based shard allocation thresholds: on large, relatively full disks (>90% used),
            # it will lead to index creation to get stuck waiting for other nodes to join the cluster,
            # which will never happen since we only have one node.
            # See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/modules-cluster.html#disk-based-shard-allocation
            - name: cluster.routing.allocation.disk.threshold_enabled
              value: 'false'
            # Disable more plugins/features that we do not use:
            - name: 'cluster.deprecation_indexing.enabled'
              value: 'false'
            - name: 'xpack.profiling.enabled'
              value: 'false'
            - name: 'xpack.ent_search.enabled'
              value: 'false'
            - name: 'indices.lifecycle.history_index_enabled'
              value: 'false'
            - name: 'slm.history_index_enabled'
              value: 'false'
            - name: 'stack.templates.enabled'
              value: 'false'
            - name: 'xpack.ml.enabled'
              value: 'false'
            - name: 'xpack.monitoring.templates.enabled'
              value: 'false'
            - name: 'xpack.watcher.enabled'
              value: 'false'
          envFrom:
            - configMapRef:
                name: search-backend-config
            - secretRef:
                name: search-backend-secrets
  volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app: search-backend
          app.kubernetes.io/name: search-backend
          app.kubernetes.io/component: datastore
          app.kubernetes.io/part-of: search-quarkus-io
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: "gp2"
        resources:
          requests:
            storage: 5Gi
